{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed0a604",
   "metadata": {},
   "source": [
    "# Using Tweetscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46583cdb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a875768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweetscrape import *\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d040d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # Load the env variables stored in local .env file\n",
    "twitter_oauth_ian  = getenv(\"TWITTER_OAUTH_IAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8178b9",
   "metadata": {},
   "source": [
    "## ScraperPopular class\n",
    "\n",
    "This class gets tweets from the endpoint, https://api.twitter.com/1.1/search/tweets.json, and requires elevated access.\n",
    "\n",
    "**Unfortunately it is hard to get a sufficient amount of tweets for a topic with this. For \"league of legends\", I got a max of 35 and for \"(elon musk OR chief twit), I got a max of like 16.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e872e8a",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e923e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_popular = ScraperPopular(twitter_oauth_ian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6c9c5",
   "metadata": {},
   "source": [
    "### Params\n",
    "\n",
    "The class is initialized with several default params. You can update these params, i.e.,\n",
    "`scraper_popular.params['lang']='jp'`. The params that most commonly vary between queries, however, can simply be passed in as arguments to the `scrape` functions (i.e., `query`, `max_results`, `lang`).\n",
    "\n",
    "You'll notice that `ScraperPopular` uses a `q` param. The older endpoints use `q` instead of query and feature a separate `lang` parameter for the language. This differs from the v2 endpoints where the query param is `query` and the language is included within the query argument (e.g. `'query': '(elon musk OR chief twit) lang:en'`)\n",
    "\n",
    "To delete a param, call the pop function. E.g.,\n",
    "`scraper_popular.params.pop('user.fields')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d766d6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': '',\n",
       " 'lang': '',\n",
       " 'max_results': '100',\n",
       " 'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
       " 'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
       " 'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
       " 'result_type': 'popular'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the params set by default. You don't have to bother changing q, lang, or max_results. These can be specified each time you call the scrape function.\n",
    "scraper_popular.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba050c68",
   "metadata": {},
   "source": [
    "### Pagination token -- get new data between queries\n",
    "\n",
    "The standard search API ScraperPopular uses returns 14 tweets per call. Therefore, to get the next page of tweets, a token has to be passed from the last result so the same tweets aren't fetched.\n",
    "\n",
    "Technically with this older endpoint, this is actually stored in the metadata array as `max_id` and if you run a query and then check the class' params (e.g., `scraper_popular.params`), you will notice a `max_id` param.\n",
    "\n",
    "**If you want to continue scraping tweets with the same query later**, get the last pagination token when your current query finishes using the getter, and save it somewhere:\n",
    "\n",
    "`scraper_popular.get_pagination_token()`\n",
    "\n",
    "When you want to continue scraping, then, if you initialize a new instance, simply set the pagination token using the setter or pass it in as an argument to the scrape function Either way, it will scrape from that point onward so you will be getting new data.\n",
    "\n",
    "`scraper_popular.set_pagination_token(\"asdfasdfasdfasdfa\")`\n",
    "\n",
    "**If you keep the same instance alive and repeat the same query, the pagination token is updating automatically** so there's no need to keep setting it each time you run the scrape function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6ec33",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "\n",
    "<span style=\"color: crimson; font-weight: bolder;\">def scrape(self, query, max_results=500, lang='en', pagination_token=''):</span>\n",
    "\n",
    "Scrapes the endpoint https://api.twitter.com/1.1/search/tweets.json with the 'popular' `result_type` parameter. Loops until the number of tweets captured reaches max_results or until no remaining tweets are retrieved.\n",
    "\n",
    "<span style=\"color: blue; font-weight: bolder;\">Args:</span><br>\n",
    "&emsp;query (str): Your search query. Can be boolean, e.g., \"('elon musk OR chief twit')\"\n",
    "<br><br>\n",
    "&emsp;max_results (int, optional): The max number of rows you want in the returned DataFrame. Defaults to 500.\n",
    "<br><br>\n",
    "&emsp;lang (str, optional): IISO2 language code for the tweets. None will remove the lang parameter. Defaults to None.\n",
    "<br><br>\n",
    "&emsp;pagination_token (str, optional): Really max_id for this endpoint. Used to set the 'max_id' param. Defaults to ''.\n",
    "<br><br>\n",
    "<span style=\"color: blue; font-weight: bolder;\">Returns:</span><br>\n",
    "&emsp;pandas.DataFrame: A DataFrame of the results.\n",
    "<br>\n",
    "\n",
    "`df1 = scraper_popular.scrape(\"league of legends\", max_results=5_000)`\n",
    "\n",
    "### Merging the dataframes from multiple scrapes:\n",
    "\n",
    "Assuming the parameters for the fields to include have not changed between two scrapes, you can merge the dataframes from those scrapes as such:\n",
    "\n",
    "```\n",
    "df1 = scraper_popular.scrape(\"league of legends\", max_results=5_000)\n",
    "df2 = scraper_popular.scrape(\"league of legends\", max_results=10_000)\n",
    "\n",
    "df3 = pd.concat([df1, df2], ignore_index=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbae6ba",
   "metadata": {},
   "source": [
    "## ScraperArchive class\n",
    "\n",
    "This class gets tweets from the endpoint, https://api.twitter.com/2/tweets/search/all, and requires academic research access.\n",
    "\n",
    "It is used the same way as ScraperPopular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80432d9c",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "\n",
    "<span style=\"color: crimson; font-weight: bolder;\">def scrape(self, query, max_results=500, lang='en', pagination_token=''):</span>\n",
    "\n",
    "Scrapes the endpoint https://api.twitter.com/1.1/search/tweets.json with the 'popular' `result_type` parameter. Loops until the number of tweets captured reaches max_results or until no remaining tweets are retrieved.\n",
    "\n",
    "<span style=\"color: blue; font-weight: bolder;\">Args:</span><br>\n",
    "&emsp;query (str): Your search query. Can be boolean, e.g., \"('elon musk OR cheif twit')\"\n",
    "<br><br>\n",
    "&emsp;max_results (int, optional): The max number of rows you want in the returned DataFrame. Defaults to 500.\n",
    "<br><br>\n",
    "&emsp;lang (str, optional): ISO2 language code for the tweets. None will disclude the lang argument. Defaults to None.\n",
    "<br><br>\n",
    "&emsp;pagination_token (str, optional): Used to set the 'pagination_token' param. Defaults to ''.\n",
    "<br><br>\n",
    "<span style=\"color: blue; font-weight: bolder;\">Returns:</span><br>\n",
    "&emsp;pandas.DataFrame: A DataFrame of the results.\n",
    "<br>\n",
    "\n",
    "`df1 = scraper_archive.scrape(\"league of legends\", max_results=5_000)`\n",
    "\n",
    "### Merging the dataframes from multiple scrapes:\n",
    "\n",
    "Assuming the parameters for the fields to include have not changed between two scrapes, you can merge the dataframes from those scrapes as such:\n",
    "\n",
    "```\n",
    "df1 = scraper_archive.scrape(\"league of legends\", max_results=5_000)\n",
    "df2 = scraper_archive.scrape(\"league of legends\", max_results=10_000)\n",
    "\n",
    "df3 = pd.concat([df1, df2], ignore_index=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a5582",
   "metadata": {},
   "source": [
    "## ScraperArchive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd7eaf",
   "metadata": {},
   "source": [
    "### League of Legends Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da836014",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_scraper = ScraperArchive(twitter_oauth_ian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fcdb3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagination token: b26v89c19zqg8o3fpzen8dk1mpzoekbfuz08yrie3okfx\n"
     ]
    }
   ],
   "source": [
    "query = 'league of legends'\n",
    "\n",
    "# Let's gather 1000 tweets and write them to a CSV file:\n",
    "league_tweets1 = archive_scraper.scrape(query=query, max_results=1000, lang='en')\n",
    "league_tweets1.to_csv(\"Datasets/ScrapedTwitter/leauge_tweets_1k.csv\") # It actually manage to gather 1099\n",
    "\n",
    "# Now lets get the pagination token for when we want to get more Musk tweets:\n",
    "pagination_token = archive_scraper.get_pagination_token()\n",
    "print(f\"Pagination token: {pagination_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62743ff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'leauge_tweets2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Let's gather 14000 MORE tweets and write them to a CSV file:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m league_tweets2 \u001b[38;5;241m=\u001b[39m archive_scraper\u001b[38;5;241m.\u001b[39mscrape(query, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14_000\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mleauge_tweets2\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleague_tweets_14k.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'leauge_tweets2' is not defined"
     ]
    }
   ],
   "source": [
    "# Pretend the old instance is disposed so the pagination\n",
    "# token has to be passed to the new instance:\n",
    "archive_scraper = ScraperArchive(twitter_oauth_ian)\n",
    "archive_scraper.set_pagination_token('b26v89c19zqg8o3fpzen8dk1mpzoekbfuz08yrie3okfx')\n",
    "\n",
    "# Let's gather 14000 MORE tweets and write them to a CSV file:\n",
    "league_tweets2 = archive_scraper.scrape(query, max_results=14_000, lang='en') # This will take HoT several minutes\n",
    "leauge_tweets2.to_csv(\"Datasets/ScrapedTwitter/league_tweets_14k.csv\") #WH000PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9498d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "league_tweets2.to_csv(\"Datasets/ScrapedTwitter/league_tweets_14k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a37dbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge the bastards into one DataFrame and export the data:\n",
    "league_tweets15k = pd.concat([league_tweets1, league_tweets2], ignore_index=True)\n",
    "league_tweets15k.to_csv(\"Datasets/ScrapedTwitter/league_tweets_15k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1819cb",
   "metadata": {},
   "source": [
    "<h3 style=\"color: red;\">Don't forget to get the pagination token!</h3>\n",
    "<p>\n",
    "This will make life easier when you want to get more tweets using the same query.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0038a148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last pagination token for query 'league of legends':\n",
      "b26v89c19zqg8o3fpzen875uz627neg8vi5f64xyb0n7h\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last pagination token for query '{query}':\")\n",
    "print(archive_scraper.get_pagination_token())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32665a1b",
   "metadata": {},
   "source": [
    "### Elon Musk Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "486a24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "musk_archive_scraper = ScraperArchive(twitter_oauth_ian)\n",
    "\n",
    "query = '(elon musk OR chief twit)'\n",
    "musk_tweets15k = musk_archive_scraper.scrape(query, max_results=15_000, lang='en') # This will take HoT several minutes\n",
    "musk_tweets15k.to_csv(\"Datasets/ScrapedTwitter/musk_tweets_15k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d1a4a",
   "metadata": {},
   "source": [
    "<h3 style=\"color: red;\">Don't forget to get the pagination token!</h3>\n",
    "<p>\n",
    "This will make life easier when you want to get more tweets using the same query.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d1d8042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last pagination token for query '(elon musk OR chief twit)':\n",
      "b26v89c19zqg8o3fpzen8fn9sdtflk0xqqfyru2wcj1bx\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last pagination token for query '{query}':\")\n",
    "print(musk_archive_scraper.get_pagination_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef164800",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dinosaur Tweets\n",
    "query = 'dinosaur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "87dae40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dinosaur Tweets\n",
    "dino_tweets15k = archive_scraper.scrape(query, max_results=15_000, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c6cbf556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "public_metrics            {'retweet_count': 0, 'reply_count': 0, 'like_c...\n",
       "edit_history_tweet_ids                                [1588245412221046784]\n",
       "conversation_id                                         1588245412221046784\n",
       "created_at                                         2022-11-03T19:03:09.000Z\n",
       "text                      Light Painting 101: Illuminating a terrifying ...\n",
       "id                                                      1588245412221046784\n",
       "referenced_tweets                                                       NaN\n",
       "author_id                                                         434534859\n",
       "lang                                                                     en\n",
       "source                                                                IFTTT\n",
       "reply_settings                                                     everyone\n",
       "in_reply_to_user_id                                                     NaN\n",
       "geo                                                                     NaN\n",
       "withheld                                                                NaN\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_tweets15k.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639fefe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug author ID into https://tweeterid.com/ --> User: @WZ65\n",
    "# --> Scrolled down their feed and found the dino tweet posted on Nov. 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f2bf0d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_tweets15k.to_csv(\"Datasets/ScrapedTwitter/dino_tweets_15k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a8f20",
   "metadata": {},
   "source": [
    "### Dinosaur -- last pagination token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3520a9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last pagination token for query 'dinosaur':\n",
      "b26v89c19zqg8o3fpzemdy1syktbx8t777oii7etrz4ot\n"
     ]
    }
   ],
   "source": [
    "print(\"Last pagination token for query 'dinosaur':\")\n",
    "print(archive_scraper.get_pagination_token())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
