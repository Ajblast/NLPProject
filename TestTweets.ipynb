{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164fffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\seth\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\seth\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seth\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\seth\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\seth\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\seth\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment\n",
    "pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28536303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib. cm as cm\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from cleantext import clean\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6fe32661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctTokens(tokens):\n",
    "    punctuation = set(string.punctuation) \n",
    "    newTokens = []\n",
    "    for token in tokens:\n",
    "        if token in punctuation:\n",
    "            continue\n",
    "        else:\n",
    "            newTokens.append(token)\n",
    "            \n",
    "def replaceWithSpaces(string, characters):\n",
    "    puncutation = set(characters)\n",
    "    outString = string\n",
    "    for character in characters:\n",
    "        outString = outString.replace(character, \" \")\n",
    "        \n",
    "    return outString\n",
    "\n",
    "def removeHtml(string):\n",
    "    outstring = re.sub(f\"<a [^<]*>\",\"\",string)\n",
    "    outstring = re.sub(f\"<\\/a>\", \"\",outstring)\n",
    "    outstring = outstring.replace(\"</a>\", \"\")\n",
    "    outstring = outstring.replace(\"<br />\", \"\")\n",
    "    return outstring\n",
    "\n",
    "def removeLink(string):\n",
    "    return re.sub(f\"https?:\\/\\/(.*?[\\s+]|.*?$)\",\" \",string)\n",
    "\n",
    "def removeAts(string):\n",
    "    return re.sub(f\"@\\w*\",\" \",string)\n",
    "\n",
    "def removeRetweet(string):\n",
    "    return re.sub(f\"^rt *:\",\"\",string)\n",
    "\n",
    "def cleanTweet(string):\n",
    "    return removeLink(removeRetweet(removeAts(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd44511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentence, embeddings):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    cleaned_tokens = cleanList(tokens) #unclear what \"special character\" are. I'm cleaning tokens of any non-alphabetic characters\n",
    "    word_embeddings = []\n",
    "    for token in cleaned_tokens:\n",
    "        if token in embeddings:\n",
    "            word_embeddings.append(embeddings[token])\n",
    "        else:\n",
    "            word_embeddings.append(np.zeros(300))\n",
    "    emb_arr = np.array(word_embeddings)\n",
    "    norm_vec = emb_arr.sum(axis=0)\n",
    "    if type(norm_vec) != np.ndarray:\n",
    "         return np.zeros(300)\n",
    "    return norm_vec / np.sqrt((norm_vec ** 2).sum())\n",
    "\n",
    "def removeNonAlphabetic(string):\n",
    "    return re.sub(\"[\\W\\d]\",\"\",string)\n",
    "\n",
    "\n",
    "def cleanTokens(tokens):\n",
    "    output = []\n",
    "    for token in tokens:\n",
    "        cleanToken = removeNonAlphabetic(token)\n",
    "        if len(cleanToken) == 0:\n",
    "            continue\n",
    "        output.append(cleanToken)\n",
    "    return output\n",
    "\n",
    "def removeStopWords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleanList = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            cleanList.append(token)\n",
    "    return cleanList\n",
    "\n",
    "def cleanList(tokens):\n",
    "    return removeStopWords(cleanTokens(tokens))\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, ax_lst = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "    \n",
    "    epochs = np.array(history.epoch) + 1\n",
    "    \n",
    "    ax_lst[0].plot(epochs, history.history['loss'], label='Training')    \n",
    "    ax_lst[0].plot(epochs, history.history['val_loss'], label='Testing')\n",
    "    ax_lst[0].set_ylabel('Loss')\n",
    "    ax_lst[0].set_xlabel('Epoch')\n",
    "    ax_lst[0].set_xticks(epochs)\n",
    "    \n",
    "    ax_lst[1].plot(epochs, history.history['accuracy'], label='Training')    \n",
    "    ax_lst[1].plot(epochs, history.history['val_accuracy'], label='Testing')\n",
    "    ax_lst[1].set_ylabel('Accuracy')\n",
    "    ax_lst[1].set_xlabel('Epoch')\n",
    "    ax_lst[1].set_xticks(epochs)\n",
    "    \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5dc6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out the pre-processed twitter tweets success rates, neutral bounds -.1 and .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2f973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1117)\n",
      "(0, 1117)\n",
      "(0, 1186)\n"
     ]
    }
   ],
   "source": [
    "negative_df = pd.read_csv(\"processedNegative.csv\")\n",
    "neutral_df = pd.read_csv(\"processedNegative.csv\")\n",
    "positive_df = pd.read_csv(\"processedPositive.csv\")\n",
    "\n",
    "print(negative_df.shape)\n",
    "print(neutral_df.shape)\n",
    "print(positive_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff603f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.612354521038496\n"
     ]
    }
   ],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "numNegative = 0\n",
    "for (columnName, columnData) in negative_df.iteritems():\n",
    "    scores = sentiment.polarity_scores(columnName)\n",
    "    if(scores['compound'] < -.1):\n",
    "        numNegative = numNegative + 1\n",
    "print(numNegative / 1117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e73311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1638316920322292\n"
     ]
    }
   ],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "numNeutral = 0\n",
    "for (columnName, columnData) in neutral_df.iteritems():\n",
    "    scores = sentiment.polarity_scores(columnName)\n",
    "    if(scores['compound'] > -.1 and scores['compound'] < .1):\n",
    "        numNeutral = numNeutral + 1\n",
    "print(numNeutral / 1117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b541a371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8782452999104745\n"
     ]
    }
   ],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "numPositive = 0\n",
    "for (columnName, columnData) in positive_df.iteritems():\n",
    "    scores = sentiment.polarity_scores(columnName)\n",
    "    if(scores['compound'] > .1):\n",
    "        numPositive = numPositive + 1\n",
    "print(numPositive / 1117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266b55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out Sentiment 140 dataset, neutral bounds, only positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449f16b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>specialOne</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score          id                          date     query       specialOne  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                data  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding =\"ISO-8859-1\", names=['score','id','date','query','specialOne','data'] )\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af473e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df[['score','data']].iloc[800000][0] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3db4b35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive140List = []\n",
    "negative140List = []\n",
    "\n",
    "for index, row in sent_df[['score','data']].iterrows():\n",
    "    if row[0] == 0:\n",
    "        negative140List.append(removeLink(row[1]))\n",
    "    elif row[0] == 4:\n",
    "        positive140List.append(removeLink(row[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38d1171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000\n",
      "800000\n"
     ]
    }
   ],
   "source": [
    "print(len(negative140List))\n",
    "print(len(positive140List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61753625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive accuracy:  0.62628375\n",
      "Negative accuracy:  0.67881125\n"
     ]
    }
   ],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "numPos = 0\n",
    "numNeg = 0\n",
    "for index, row in sent_df[['score','data']].iterrows():\n",
    "    scores = sentiment.polarity_scores(row[1])\n",
    "    if(scores['compound'] > 0 and row[0] == 4):\n",
    "        numPos = numPos + 1\n",
    "    if(scores['compound'] <= 0 and row[0] == 0 ):\n",
    "        numNeg = numNeg + 1\n",
    "print(\"Positive accuracy: \", numPos / len(positive140List))\n",
    "print(\"Negative accuracy: \", numNeg / len(negative140List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d24d92cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.4939}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.polarity_scores(negative140List[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b339fa97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing out Testing out twitter sentiment analysis dataset, neutral bounds -.1 and .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb2f0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pd.read_csv(\"twitter_training.csv\", names=['id','topic','sentiment','data'] )\n",
    "twitter_df.drop(twitter_df[pd.isna(twitter_df['data'])].index,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d532092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id        topic sentiment  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                                data  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()\n",
    "#this dataset contains duplicates phrased differently, which will give us more data on how well vader can handle alternate phrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c945f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveList = []\n",
    "negativeList = []\n",
    "neutralList = []\n",
    "\n",
    "for index, row in twitter_df[['sentiment','data']].iterrows():\n",
    "    if row[0] == 'Positive':\n",
    "        positiveList.append(row[1])\n",
    "    elif row[0] == 'Negative':\n",
    "        negativeList.append(row[1])\n",
    "    elif row[0] == 'Neutral':\n",
    "        neutralList.append(row[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08f8aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22358\n",
      "20655\n",
      "18108\n"
     ]
    }
   ],
   "source": [
    "print(len(negativeList))\n",
    "print(len(positiveList))\n",
    "print(len(neutralList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f69e37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative accuracy:  0.7354414527238572\n",
      "Positive accuracy:  0.6761074800290486\n"
     ]
    }
   ],
   "source": [
    "#Only score pos neg with no neutral bounds first - give -0s to neg\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "numPos = 0\n",
    "numNeg = 0\n",
    "for item in negativeList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] <= 0):\n",
    "        numNeg = numNeg + 1   \n",
    "\n",
    "for item in positiveList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] > 0):\n",
    "        numPos = numPos + 1   \n",
    "        \n",
    "print(\"Negative accuracy: \", numNeg / len(negativeList))\n",
    "print(\"Positive accuracy: \", numPos / len(positiveList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46be65a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative accuracy:  0.5818946238482869\n",
      "Positive accuracy:  0.8254659888646817\n"
     ]
    }
   ],
   "source": [
    "#Only score pos neg with no neutral bounds first - give 0s to pos\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "numPos = 0\n",
    "numNeg = 0\n",
    "for item in negativeList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] < 0):\n",
    "        numNeg = numNeg + 1   \n",
    "\n",
    "for item in positiveList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] >= 0):\n",
    "        numPos = numPos + 1   \n",
    "        \n",
    "print(\"Negative accuracy: \", numNeg / len(negativeList))\n",
    "print(\"Positive accuracy: \", numPos / len(positiveList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baf539b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative accuracy:  0.5736201806959478\n",
      "Positive accuracy:  0.6714112805616074\n",
      "Neutral accuracy:  0.2109564833222885\n"
     ]
    }
   ],
   "source": [
    "#Only score pos neg with .05 neutral bounds\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "numPos = 0\n",
    "numNeg = 0\n",
    "numNeu = 0\n",
    "for item in negativeList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] < -.05):\n",
    "        numNeg = numNeg + 1   \n",
    "\n",
    "for item in positiveList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] > .05):\n",
    "        numPos = numPos + 1   \n",
    "        \n",
    "for item in neutralList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] >= -.05 and scores['compound'] <= .05):\n",
    "        numNeu = numNeu + 1   \n",
    "\n",
    "print(\"Negative accuracy: \", numNeg / len(negativeList))\n",
    "print(\"Positive accuracy: \", numPos / len(positiveList))\n",
    "print(\"Neutral accuracy: \", numNeu / len(neutralList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1ca1cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative accuracy:  0.5736201806959478\n",
      "Positive accuracy:  0.6714112805616074\n",
      "Neutral accuracy:  0.22978793903247183\n"
     ]
    }
   ],
   "source": [
    "#Only score pos neg with .1 neutral bounds\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "numPos = 0\n",
    "numNeg = 0\n",
    "numNeu = 0\n",
    "for item in negativeList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] < -.05):\n",
    "        numNeg = numNeg + 1   \n",
    "\n",
    "for item in positiveList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] > .05):\n",
    "        numPos = numPos + 1   \n",
    "        \n",
    "for item in neutralList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] >= -.1 and scores['compound'] <= .1):\n",
    "        numNeu = numNeu + 1   \n",
    "\n",
    "print(\"Negative accuracy: \", numNeg / len(negativeList))\n",
    "print(\"Positive accuracy: \", numPos / len(positiveList))\n",
    "print(\"Neutral accuracy: \", numNeu / len(neutralList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd7a4350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num neg:  30969\n",
      "num pos:  31029\n"
     ]
    }
   ],
   "source": [
    "#carblac dataset https://huggingface.co/datasets/carblacac/twitter-sentiment-analysis\n",
    "testTweets = open('test_62k.txt', 'r', encoding='utf-8')\n",
    "Lines = testTweets.readlines()\n",
    "\n",
    "negCarList = []\n",
    "posCarList = []\n",
    "for line in Lines:\n",
    "    values = line.split(\"\\t\")\n",
    "    if values[0] == \"1\":\n",
    "        posCarList.append(values[1])\n",
    "    else:\n",
    "        negCarList.append(values[1])\n",
    "print(\"num neg: \", len(negCarList))\n",
    "print(\"num pos: \", len(posCarList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26913da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg accuracy:  0.4336917562724014\n",
      "Pos accuracy:  0.6244158690257501\n"
     ]
    }
   ],
   "source": [
    "identifiedPos = 0\n",
    "identifiedNeg = 0\n",
    "\n",
    "for item in negCarList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] < 0):\n",
    "        identifiedNeg = identifiedNeg + 1   \n",
    "\n",
    "for item in posCarList:\n",
    "    scores = sentiment.polarity_scores(item)\n",
    "    if(scores['compound'] > 0):\n",
    "        identifiedPos = identifiedPos + 1\n",
    "        \n",
    "print(\"Neg accuracy: \", identifiedNeg / len(negCarList))\n",
    "print(\"Pos accuracy: \", identifiedPos / len(posCarList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72a9af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Things look bad - let's make a NN\n",
    "gloveFile = open('glove.6B.300d.txt', 'r', encoding='utf-8')\n",
    "\n",
    "embeddings_index = {}\n",
    "for line in gloveFile.readlines():\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = [float(value) for value in values[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abe78b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seth\\AppData\\Local\\Temp/ipykernel_4940/704902115.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return norm_vec / np.sqrt((norm_vec ** 2).sum())\n"
     ]
    }
   ],
   "source": [
    "testTweets = open('train_150k.txt', 'r', encoding='utf-8')\n",
    "Lines = testTweets.readlines()\n",
    "cleanTweets = []\n",
    "scores = []\n",
    "\n",
    "for line in Lines:\n",
    "    values = line.split(\"\\t\")\n",
    "    scores.append(int(values[0]))\n",
    "    cleanTweets.append(cleanTweet(values[1]))\n",
    "\n",
    "\n",
    "tweetGlove = np.array([sent2vec(tweet,embeddings_index) for tweet in cleanTweets])\n",
    "tweetGlove = np.nan_to_num(tweetGlove)  \n",
    "\n",
    "npScores = np.array(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b8b2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tweetGlove,npScores,test_size=.2,random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46f33a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                19264     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,889\n",
      "Trainable params: 21,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "4800/4800 [==============================] - 5s 975us/step - loss: 0.5614 - accuracy: 0.7091 - val_loss: 0.5369 - val_accuracy: 0.7271\n",
      "Epoch 2/20\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.5398 - accuracy: 0.7266 - val_loss: 0.5275 - val_accuracy: 0.7353\n",
      "Epoch 3/20\n",
      "4800/4800 [==============================] - 4s 895us/step - loss: 0.5304 - accuracy: 0.7339 - val_loss: 0.5273 - val_accuracy: 0.7388\n",
      "Epoch 4/20\n",
      "4800/4800 [==============================] - 4s 904us/step - loss: 0.5240 - accuracy: 0.7389 - val_loss: 0.5224 - val_accuracy: 0.7356\n",
      "Epoch 5/20\n",
      "4800/4800 [==============================] - 4s 893us/step - loss: 0.5187 - accuracy: 0.7424 - val_loss: 0.5202 - val_accuracy: 0.7370\n",
      "Epoch 6/20\n",
      "4800/4800 [==============================] - 4s 898us/step - loss: 0.5140 - accuracy: 0.7460 - val_loss: 0.5161 - val_accuracy: 0.7443\n",
      "Epoch 7/20\n",
      "4800/4800 [==============================] - 4s 908us/step - loss: 0.5102 - accuracy: 0.7495 - val_loss: 0.5146 - val_accuracy: 0.7442\n",
      "Epoch 8/20\n",
      "4800/4800 [==============================] - 4s 903us/step - loss: 0.5069 - accuracy: 0.7513 - val_loss: 0.5151 - val_accuracy: 0.7444\n",
      "Epoch 9/20\n",
      "4800/4800 [==============================] - 4s 916us/step - loss: 0.5038 - accuracy: 0.7530 - val_loss: 0.5150 - val_accuracy: 0.7448\n",
      "Epoch 10/20\n",
      "4800/4800 [==============================] - 4s 917us/step - loss: 0.5005 - accuracy: 0.7558 - val_loss: 0.5129 - val_accuracy: 0.7466\n",
      "Epoch 11/20\n",
      "4800/4800 [==============================] - 4s 923us/step - loss: 0.4982 - accuracy: 0.7573 - val_loss: 0.5141 - val_accuracy: 0.7452\n",
      "Epoch 12/20\n",
      "4800/4800 [==============================] - 4s 911us/step - loss: 0.4962 - accuracy: 0.7588 - val_loss: 0.5132 - val_accuracy: 0.7459\n",
      "Epoch 13/20\n",
      "4800/4800 [==============================] - 4s 896us/step - loss: 0.4936 - accuracy: 0.7597 - val_loss: 0.5140 - val_accuracy: 0.7457\n",
      "Epoch 14/20\n",
      "4800/4800 [==============================] - 4s 898us/step - loss: 0.4913 - accuracy: 0.7619 - val_loss: 0.5129 - val_accuracy: 0.7449\n",
      "Epoch 15/20\n",
      "4800/4800 [==============================] - 4s 912us/step - loss: 0.4895 - accuracy: 0.7625 - val_loss: 0.5200 - val_accuracy: 0.7407\n",
      "Epoch 16/20\n",
      "4800/4800 [==============================] - 4s 901us/step - loss: 0.4890 - accuracy: 0.7634 - val_loss: 0.5159 - val_accuracy: 0.7431\n",
      "Epoch 17/20\n",
      "4800/4800 [==============================] - 4s 906us/step - loss: 0.4864 - accuracy: 0.7656 - val_loss: 0.5135 - val_accuracy: 0.7435\n",
      "Epoch 18/20\n",
      "4800/4800 [==============================] - 4s 902us/step - loss: 0.4857 - accuracy: 0.7654 - val_loss: 0.5138 - val_accuracy: 0.7429\n",
      "Epoch 19/20\n",
      "4800/4800 [==============================] - 4s 905us/step - loss: 0.4841 - accuracy: 0.7667 - val_loss: 0.5168 - val_accuracy: 0.7434\n",
      "Epoch 20/20\n",
      "4800/4800 [==============================] - 4s 906us/step - loss: 0.4823 - accuracy: 0.7679 - val_loss: 0.5154 - val_accuracy: 0.7432\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                   validation_data=(x_test, y_test),\n",
    "                   epochs=20,\n",
    "                   batch_size=25,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e286c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e9f4e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seth\\AppData\\Local\\Temp/ipykernel_4940/704902115.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return norm_vec / np.sqrt((norm_vec ** 2).sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61998, 300)\n",
      "1938/1938 [==============================] - 1s 468us/step\n",
      "0.7381689731926836\n"
     ]
    }
   ],
   "source": [
    "testTweets = open('test_62k.txt', 'r', encoding='utf-8')\n",
    "Lines = testTweets.readlines()\n",
    "testScores = []\n",
    "cleanTestTweets= []\n",
    "\n",
    "for line in Lines:\n",
    "    values = line.split(\"\\t\")\n",
    "    testScores.append(int(values[0]))\n",
    "    cleanTestTweets.append(cleanTweet(values[1]))\n",
    "\n",
    "testGlove = np.array([sent2vec(tweet,embeddings_index) for tweet in cleanTestTweets])\n",
    "testGlove = np.nan_to_num(testGlove)  \n",
    "print(testGlove.shape)\n",
    "prediction = model.predict(testGlove)\n",
    "counter = 0\n",
    "correctGuesses = 0\n",
    "for score in testScores:\n",
    "    if score == 1 and prediction[counter] > .5:\n",
    "        correctGuesses = correctGuesses + 1\n",
    "    elif score == 0 and prediction[counter] <= .5:\n",
    "        correctGuesses = correctGuesses + 1\n",
    "    counter = counter + 1\n",
    "\n",
    "print(correctGuesses/len(testScores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25bf3b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1938/1938 [==============================] - 1s 502us/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(testGlove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad48da97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0] > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338fb637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ceb98bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding =\"ISO-8859-1\", names=['score','id','date','query','specialOne','data'] )\n",
    "\n",
    "testTweets = open('train_150k.txt', 'r', encoding='utf-8')\n",
    "Lines = testTweets.readlines()\n",
    "\n",
    "compiledTweets = []\n",
    "compiledScores = []\n",
    "\n",
    "#Append 160000 processed no emoticon\n",
    "for index, row in sent_df[['score','data']].iterrows():\n",
    "    if row[0] == 0:\n",
    "        compiledTweets.append(cleanTweet(row[1]))\n",
    "        compiledScores.append(0)\n",
    "    elif row[0] == 4:\n",
    "        compiledTweets.append(cleanTweet(row[1]))\n",
    "        compiledScores.append(1)\n",
    "\n",
    "#append cbac train tweets  \n",
    "for line in Lines:\n",
    "    values = line.split(\"\\t\")\n",
    "    if values[0] == \"1\":\n",
    "        compiledTweets.append(cleanTweet(values[1]))\n",
    "        compiledScores.append(1)\n",
    "    else:\n",
    "        compiledTweets.append(cleanTweet(values[1]))\n",
    "        compiledScores.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cddbea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1749985\n",
      "1749985\n"
     ]
    }
   ],
   "source": [
    "print(len(compiledTweets))\n",
    "print(len(compiledScores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd9b01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Things look bad - let's make a NN\n",
    "gloveFile = open('glove.6B.300d.txt', 'r', encoding='utf-8')\n",
    "\n",
    "embeddings_index = {}\n",
    "for line in gloveFile.readlines():\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = [float(value) for value in values[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f21d7eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seth\\AppData\\Local\\Temp/ipykernel_4940/704902115.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return norm_vec / np.sqrt((norm_vec ** 2).sum())\n"
     ]
    }
   ],
   "source": [
    "tweetGlove = np.array([sent2vec(tweet,embeddings_index) for tweet in compiledTweets])\n",
    "tweetGlove = np.nan_to_num(tweetGlove)  \n",
    "\n",
    "npCompiledScores = np.array(compiledScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87d477b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tweetGlove,npCompiledScores,test_size=.2,random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7f4d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 128)               38528     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,409\n",
      "Trainable params: 49,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "56000/56000 [==============================] - 62s 1ms/step - loss: 0.5260 - accuracy: 0.7361 - val_loss: 0.5055 - val_accuracy: 0.7502\n",
      "Epoch 2/20\n",
      "56000/56000 [==============================] - 62s 1ms/step - loss: 0.5091 - accuracy: 0.7482 - val_loss: 0.4975 - val_accuracy: 0.7563\n",
      "Epoch 3/20\n",
      "56000/56000 [==============================] - 63s 1ms/step - loss: 0.5030 - accuracy: 0.7523 - val_loss: 0.4976 - val_accuracy: 0.7586\n",
      "Epoch 4/20\n",
      "56000/56000 [==============================] - 64s 1ms/step - loss: 0.4995 - accuracy: 0.7547 - val_loss: 0.4937 - val_accuracy: 0.7602\n",
      "Epoch 5/20\n",
      "56000/56000 [==============================] - 65s 1ms/step - loss: 0.4970 - accuracy: 0.7563 - val_loss: 0.4918 - val_accuracy: 0.7604\n",
      "Epoch 6/20\n",
      "56000/56000 [==============================] - 66s 1ms/step - loss: 0.4952 - accuracy: 0.7576 - val_loss: 0.4946 - val_accuracy: 0.7624\n",
      "Epoch 7/20\n",
      "56000/56000 [==============================] - 65s 1ms/step - loss: 0.4938 - accuracy: 0.7586 - val_loss: 0.4895 - val_accuracy: 0.7621\n",
      "Epoch 8/20\n",
      "56000/56000 [==============================] - 68s 1ms/step - loss: 0.4926 - accuracy: 0.7594 - val_loss: 0.4913 - val_accuracy: 0.7613\n",
      "Epoch 9/20\n",
      "56000/56000 [==============================] - 66s 1ms/step - loss: 0.4916 - accuracy: 0.7601 - val_loss: 0.4884 - val_accuracy: 0.7634\n",
      "Epoch 10/20\n",
      "56000/56000 [==============================] - 67s 1ms/step - loss: 0.4905 - accuracy: 0.7603 - val_loss: 0.4872 - val_accuracy: 0.7634\n",
      "Epoch 11/20\n",
      "56000/56000 [==============================] - 70s 1ms/step - loss: 0.4900 - accuracy: 0.7609 - val_loss: 0.4916 - val_accuracy: 0.7623\n",
      "Epoch 12/20\n",
      "56000/56000 [==============================] - 71s 1ms/step - loss: 0.4892 - accuracy: 0.7619 - val_loss: 0.4867 - val_accuracy: 0.7632\n",
      "Epoch 13/20\n",
      "56000/56000 [==============================] - 74s 1ms/step - loss: 0.4889 - accuracy: 0.7622 - val_loss: 0.4848 - val_accuracy: 0.7645\n",
      "Epoch 14/20\n",
      "56000/56000 [==============================] - 76s 1ms/step - loss: 0.4881 - accuracy: 0.7625 - val_loss: 0.4866 - val_accuracy: 0.7640\n",
      "Epoch 15/20\n",
      "56000/56000 [==============================] - 83s 1ms/step - loss: 0.4875 - accuracy: 0.7630 - val_loss: 0.4862 - val_accuracy: 0.7648\n",
      "Epoch 16/20\n",
      "56000/56000 [==============================] - 93s 2ms/step - loss: 0.4868 - accuracy: 0.7636 - val_loss: 0.4886 - val_accuracy: 0.7653\n",
      "Epoch 17/20\n",
      "56000/56000 [==============================] - 92s 2ms/step - loss: 0.4864 - accuracy: 0.7639 - val_loss: 0.4853 - val_accuracy: 0.7646\n",
      "Epoch 18/20\n",
      "56000/56000 [==============================] - 96s 2ms/step - loss: 0.4861 - accuracy: 0.7638 - val_loss: 0.4853 - val_accuracy: 0.7655\n",
      "Epoch 19/20\n",
      "56000/56000 [==============================] - 98s 2ms/step - loss: 0.4854 - accuracy: 0.7643 - val_loss: 0.4865 - val_accuracy: 0.7656\n",
      "Epoch 20/20\n",
      "56000/56000 [==============================] - 99s 2ms/step - loss: 0.4851 - accuracy: 0.7647 - val_loss: 0.4859 - val_accuracy: 0.7648\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                   validation_data=(x_test, y_test),\n",
    "                   epochs=20,\n",
    "                   batch_size=25,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd6bb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking every dataset, pos neg only, embed them, 0 negative, 1 positive\n",
    "negative_df = pd.read_csv(\"processedNegative.csv\") #exclude\n",
    "positive_df = pd.read_csv(\"processedPositive.csv\") #exclude\n",
    "\n",
    "twitter_df = pd.read_csv(\"twitter_training.csv\", names=['id','topic','sentiment','data'] )#exclude\n",
    "twitter_df.drop(twitter_df[pd.isna(twitter_df['data'])].index,inplace = True) #exclude\n",
    "\n",
    "\n",
    "for (columnName, columnData) in positive_df.iteritems():\n",
    "    compiledTweets.append(cleanTweet(columnName))\n",
    "    compiledScores.append(1)\n",
    "    \n",
    "for (columnName, columnData) in negative_df.iteritems():\n",
    "    compiledTweets.append(cleanTweet(columnName))\n",
    "    compiledScores.append(0)\n",
    "\n",
    "for index, row in twitter_df[['sentiment','data']].iterrows():\n",
    "    if row[0] == 'Positive':\n",
    "        compiledTweets.append(cleanTweet(row[1]))\n",
    "        compiledScores.append(1)\n",
    "    elif row[0] == 'Negative':\n",
    "        compiledTweets.append(cleanTweet(row[1]))\n",
    "        compiledScores.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85dfb84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seth\\AppData\\Local\\Temp/ipykernel_4940/704902115.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return norm_vec / np.sqrt((norm_vec ** 2).sum())\n"
     ]
    }
   ],
   "source": [
    "tweetGlove = np.array([sent2vec(tweet,embeddings_index) for tweet in compiledTweets])\n",
    "tweetGlove = np.nan_to_num(tweetGlove)  \n",
    "\n",
    "npCompiledScores = np.array(compiledScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f113a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tweetGlove,npCompiledScores,test_size=.2,random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6628f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 128)               38528     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,409\n",
      "Trainable params: 49,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "57450/57450 [==============================] - 105s 2ms/step - loss: 0.5264 - accuracy: 0.7360 - val_loss: 0.5053 - val_accuracy: 0.7511\n",
      "Epoch 2/20\n",
      "57450/57450 [==============================] - 100s 2ms/step - loss: 0.5099 - accuracy: 0.7481 - val_loss: 0.5003 - val_accuracy: 0.7553\n",
      "Epoch 3/20\n",
      "57450/57450 [==============================] - 100s 2ms/step - loss: 0.5042 - accuracy: 0.7519 - val_loss: 0.4958 - val_accuracy: 0.7560\n",
      "Epoch 4/20\n",
      "57450/57450 [==============================] - 101s 2ms/step - loss: 0.5008 - accuracy: 0.7544 - val_loss: 0.4952 - val_accuracy: 0.7586\n",
      "Epoch 5/20\n",
      "57450/57450 [==============================] - 108s 2ms/step - loss: 0.4983 - accuracy: 0.7563 - val_loss: 0.4918 - val_accuracy: 0.7599\n",
      "Epoch 6/20\n",
      "57450/57450 [==============================] - 102s 2ms/step - loss: 0.4966 - accuracy: 0.7574 - val_loss: 0.4933 - val_accuracy: 0.7612\n",
      "Epoch 7/20\n",
      "57450/57450 [==============================] - 99s 2ms/step - loss: 0.4954 - accuracy: 0.7579 - val_loss: 0.4891 - val_accuracy: 0.7616\n",
      "Epoch 8/20\n",
      "57450/57450 [==============================] - 102s 2ms/step - loss: 0.4937 - accuracy: 0.7591 - val_loss: 0.4892 - val_accuracy: 0.7623\n",
      "Epoch 9/20\n",
      "57450/57450 [==============================] - 106s 2ms/step - loss: 0.4926 - accuracy: 0.7600 - val_loss: 0.4894 - val_accuracy: 0.7623\n",
      "Epoch 10/20\n",
      "57450/57450 [==============================] - 100s 2ms/step - loss: 0.4919 - accuracy: 0.7606 - val_loss: 0.4903 - val_accuracy: 0.7623\n",
      "Epoch 11/20\n",
      "57450/57450 [==============================] - 99s 2ms/step - loss: 0.4911 - accuracy: 0.7609 - val_loss: 0.4888 - val_accuracy: 0.7632\n",
      "Epoch 12/20\n",
      "57450/57450 [==============================] - 100s 2ms/step - loss: 0.4902 - accuracy: 0.7617 - val_loss: 0.4886 - val_accuracy: 0.7629\n",
      "Epoch 13/20\n",
      "57450/57450 [==============================] - 112s 2ms/step - loss: 0.4895 - accuracy: 0.7621 - val_loss: 0.4870 - val_accuracy: 0.7621\n",
      "Epoch 14/20\n",
      "57450/57450 [==============================] - 109s 2ms/step - loss: 0.4891 - accuracy: 0.7626 - val_loss: 0.4886 - val_accuracy: 0.7627\n",
      "Epoch 15/20\n",
      "57450/57450 [==============================] - 105s 2ms/step - loss: 0.4883 - accuracy: 0.7631 - val_loss: 0.4879 - val_accuracy: 0.7616\n",
      "Epoch 16/20\n",
      "57450/57450 [==============================] - 105s 2ms/step - loss: 0.4880 - accuracy: 0.7634 - val_loss: 0.4874 - val_accuracy: 0.7636\n",
      "Epoch 17/20\n",
      "57450/57450 [==============================] - 106s 2ms/step - loss: 0.4872 - accuracy: 0.7639 - val_loss: 0.4919 - val_accuracy: 0.7622\n",
      "Epoch 18/20\n",
      "57450/57450 [==============================] - 104s 2ms/step - loss: 0.4873 - accuracy: 0.7639 - val_loss: 0.4846 - val_accuracy: 0.7642\n",
      "Epoch 19/20\n",
      "57450/57450 [==============================] - 103s 2ms/step - loss: 0.4865 - accuracy: 0.7647 - val_loss: 0.4863 - val_accuracy: 0.7648\n",
      "Epoch 20/20\n",
      "57450/57450 [==============================] - 104s 2ms/step - loss: 0.4862 - accuracy: 0.7648 - val_loss: 0.4852 - val_accuracy: 0.7646\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                   validation_data=(x_test, y_test),\n",
    "                   epochs=20,\n",
    "                   batch_size=25,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eca3933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('17MilTweetNN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02790fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x227de66c190>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('17MilTweetNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "930278c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11221/11221 [==============================] - 19s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "values = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "662b1f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16694458], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "289a4040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     RT @purinkoo: jungkookie is really the pretty ...\n",
       "1     Light Painting 101: Illuminating a terrifying ...\n",
       "2     Light Painting 101: Illuminating a terrifying ...\n",
       "3     RT @Coolio_Art: Small art tip for drawing rept...\n",
       "4     RT @nahi_tigray: There is never true peace wit...\n",
       "5     RT @Coolio_Art: Small art tip for drawing rept...\n",
       "6     RT @purinkoo: jungkookie is really the pretty ...\n",
       "7     RT @Coolio_Art: Small art tip for drawing rept...\n",
       "8     RT @mulu96: @hrw There is never true peace wit...\n",
       "9     @JRPTO @esjyt11 @Texan81 @Lauren_Southern And ...\n",
       "10    RT @JurassicAddict: Man this year really felt ...\n",
       "11    A fossil unearthed on Koh Por island in Koh Ko...\n",
       "12       @gendernudetral Ooooooh its the dinosaur thing\n",
       "13    RT @realrkwest: @ThatEricAlper Teletubbies, Ba...\n",
       "14    RT @Coolio_Art: Small art tip for drawing rept...\n",
       "15    RT @mulu96: @hrw There is never true peace wit...\n",
       "16    RT @mulu96: @hrw There is never true peace wit...\n",
       "17                  @_krystoast The dinosaur! Raaawr! 🦖\n",
       "18    Please stop trying to make dinosaur cakes. htt...\n",
       "19    RT @letushTigray: There is never true peace wi...\n",
       "20    RT @GeologyTime: Fabulous Agatized Dinosaur Bo...\n",
       "21    RT @Coolio_Art: Small art tip for drawing rept...\n",
       "22    RT @haytimmie: Every year we do @Dinovember, t...\n",
       "23    Of this psych out the little Dinosaur and his ...\n",
       "24    RT @_LordKhaled: 👑 #DadandSon 👑\\n❤ 👴🏻 vs 👦🏻 ❤\\...\n",
       "25    ‼NEW &amp; UPDATED ONLINE MULTIPLAYER‼ \\nDinos...\n",
       "26    🦕“Principal Dina” is a 100-year-old puppet din...\n",
       "27    @LaurieGoulding So runeterra does have a cente...\n",
       "28    RT @JohnsonJeffro: People argued that stuff li...\n",
       "29    RT @JosephPinion: 🚨 NEW AD ALERT🚨 \\n\\nChuck Sc...\n",
       "30    @saddymayo dinosaur kid, mosasaurus reigns sup...\n",
       "31    Why is Devil Dinosaur, a character no one care...\n",
       "32                           Dinosaur is kinda not good\n",
       "33    Yes you didn't deliver  take back control you ...\n",
       "34    RT @DinosaurEarth: @elonmusk Very interesting....\n",
       "35    RT @nahi_tigray: There is never true peace wit...\n",
       "36    @Strandjunker They voted blue in 2020 and look...\n",
       "37    RT @mulu96: @hrw There is never true peace wit...\n",
       "38    RT @saddymayo: We’re you a dinosaur kid, a spa...\n",
       "39    RT @GimmieTheJimmie: Giganotosaurus!  I had fu...\n",
       "40    RT @nahi_tigray: There is never true peace wit...\n",
       "41    RT @mulu96: @hrw There is never true peace wit...\n",
       "42    @ThatEricAlper Teletubbies, Barney the purple ...\n",
       "43    RT @Coolio_Art: Small art tip for drawing rept...\n",
       "44    RT @Coolio_Art: Small art tip for drawing rept...\n",
       "45    RT @Lighterium: Generic bird of prey colored d...\n",
       "46    RT @Coolio_Art: Small art tip for drawing rept...\n",
       "47    First tweet, idk what to say...take a look at ...\n",
       "48    There is never true peace without justice. @Ab...\n",
       "49    @PhillipAdams_1 As we are of you, boring old c...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_df = pd.read_csv(\"RyanData/dino_tweets_15k.csv\")\n",
    "lol_df = pd.read_csv(\"RyanData/league_tweets_15k.csv\")\n",
    "musk_df = pd.read_csv(\"RyanData/musk_tweets_15k.csv\")\n",
    "dino_df['text'][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "509bd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_df['cleanText'] = [cleanTweet(clean(text,no_emoji=True)) for text in dino_df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dcb5b721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seth\\AppData\\Local\\Temp/ipykernel_4940/704902115.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return norm_vec / np.sqrt((norm_vec ** 2).sum())\n"
     ]
    }
   ],
   "source": [
    "dinoGlove = np.array([sent2vec(tweet,embeddings_index) for tweet in dino_df['cleanText']])\n",
    "dinoGlove = np.nan_to_num(dinoGlove)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c58a6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol_df['cleanText'] = [cleanTweet(clean(text,no_emoji=True)) for text in lol_df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ca5a650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lolGlove = np.array([sent2vec(tweet,embeddings_index) for tweet in lol_df['cleanText']])\n",
    "lolGlove = np.nan_to_num(lolGlove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e9726f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "musk_df['cleanText'] = [cleanTweet(clean(text,no_emoji=True)) for text in musk_df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "10fa1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "muskGlove = np.array([sent2vec(tweet,embeddings_index) for tweet in musk_df['cleanText']])\n",
    "muskGlove = np.nan_to_num(muskGlove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "780974e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474/474 [==============================] - 1s 2ms/step\n",
      "474/474 [==============================] - 1s 2ms/step\n",
      "472/472 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "dinoResults = model.predict(dinoGlove)\n",
    "lolResults = model.predict(lolGlove)\n",
    "muskResults = model.predict(muskGlove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e0b90f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreValue(value):\n",
    "    if value >= .5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def getNumPositive(results):\n",
    "    counter = 0\n",
    "    for result in results:\n",
    "        if scoreValue(result):\n",
    "            counter = counter + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "266986ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6749554778708529\n",
      "0.8742084432717678\n",
      "0.6058113307682101\n"
     ]
    }
   ],
   "source": [
    "posDino = getNumPositive(dinoResults)\n",
    "posLol = getNumPositive(lolResults)\n",
    "posMusk = getNumPositive(muskResults)\n",
    "\n",
    "print(posDino / len(dinoGlove))\n",
    "print(posLol / len(lolGlove))\n",
    "print(posMusk / len(muskGlove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d115a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.052px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
