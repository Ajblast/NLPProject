{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c7d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04725e2",
   "metadata": {},
   "source": [
    "# Define Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc2001a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'twitter', 'semeval'\n",
    "\n",
    "dataset_type = 'twitter'\n",
    "#dataset_type = 'semeval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3ec4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "if dataset_type == 'twitter':\n",
    "    relative_import_path = 'Datasets/ScrapedTwitter/'\n",
    "    relative_export_path = 'Datasets/TwitterCleaned/'\n",
    "\n",
    "    dataset_files = [\n",
    "        'dino_tweets_15k.csv', 'league_tweets_15k.csv', 'musk_tweets_15k.csv',\n",
    "        'more_dino_tweets_15k.csv', 'more_league_tweets_15k.csv', 'more_chief_twit_15k.csv',\n",
    "        'puppies_15k.csv', '2022_supreme_court_tweets_15k.csv', '2022_imac_tweets_15k.csv',\n",
    "    ]\n",
    "    names=None\n",
    "    sep=','\n",
    "    header='infer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69a38d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semeval\n",
    "if dataset_type == 'semeval':\n",
    "    relative_import_path = 'Datasets/semeval-datasets/2017_English_final/GOLD/Subtask_A/'\n",
    "    relative_export_path = 'Datasets/SemevalCleaned/'\n",
    "    names = ['id', 'sentiment', 'tweet', 'blah']\n",
    "    sep='\\t'\n",
    "    header=None\n",
    "\n",
    "    # Get the file names\n",
    "    import_files = glob.glob(os.path.join(relative_import_path, 'twitter*.txt'))\n",
    "\n",
    "    # Get the dataset file names\n",
    "    dataset_files = []\n",
    "    for file in import_files:\n",
    "        (head, tail) = os.path.split(file)\n",
    "        dataset_files.append(tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7a90c",
   "metadata": {},
   "source": [
    "# Define text cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4256d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctTokens(tokens):\n",
    "    punctuation = set(string.punctuation) \n",
    "    newTokens = []\n",
    "    for token in tokens:\n",
    "        if token in punctuation:\n",
    "            continue\n",
    "        else:\n",
    "            newTokens.append(token)\n",
    "            \n",
    "def replaceWithSpaces(string, characters):\n",
    "    puncutation = set(characters)\n",
    "    outString = string\n",
    "    for character in characters:\n",
    "        outString = outString.replace(character, ' ')\n",
    "        \n",
    "    return outString\n",
    "\n",
    "def removeHtml(string):\n",
    "    outstring = re.sub('<a [^<]*>', '', string)\n",
    "    outstring = re.sub('<\\/a>', '', outstring)\n",
    "    outstring = outstring.replace('</a>', '')\n",
    "    outstring = outstring.replace('<br />', '')\n",
    "    \n",
    "    return outstring\n",
    "\n",
    "def removeStartingRetweet(string):\n",
    "    return re.sub('^RT @.*?: ', '', string)\n",
    "\n",
    "def removeLink(string):\n",
    "    pattern = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    \n",
    "    return re.sub(pattern, ' ', string)\n",
    "\n",
    "def removeAts(string):\n",
    "    return re.sub('@\\w*', ' ', string)\n",
    "\n",
    "def removeHashtags(string):\n",
    "    return re.sub('#\\w*', ' ', string)\n",
    "\n",
    "def collapseWhitespace(string):\n",
    "    return re.sub('\\s+', ' ', string)\n",
    "\n",
    "def cleanDirtyWebText(string):\n",
    "    return clean(string, no_emoji=True, no_emails=True, no_phone_numbers=True, replace_with_email=' ', replace_with_phone_number=' ')\n",
    "\n",
    "def cleanTweet(string):\n",
    "    string = removeStartingRetweet(string)\n",
    "    string = cleanDirtyWebText(string)\n",
    "    string = removeAts(string)\n",
    "    string = removeLink(string)\n",
    "    string = removeHashtags(string)\n",
    "    string = collapseWhitespace(string)\n",
    "    \n",
    "    # Remove starting and ending blank character \n",
    "    if len(string) >= 1 and string[0] == ' ':\n",
    "        string = string[1:]\n",
    "    if len(string) >= 1 and string[-1] == ' ':\n",
    "        string = string[:-1]\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eeb34c",
   "metadata": {},
   "source": [
    "# Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6f49d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import dino_tweets_15k.csv\n",
      "Dropped 0 null rows\n",
      "\n",
      "Import league_tweets_15k.csv\n",
      "Dropped 0 null rows\n",
      "\n",
      "Import musk_tweets_15k.csv\n",
      "Dropped 0 null rows\n",
      "\n",
      "Import more_dino_tweets_15k.csv\n",
      "Dropped 0 null rows\n",
      "\n",
      "Import more_league_tweets_15k.csv\n",
      "Dropped 0 null rows\n",
      "\n",
      "Import more_chief_twit_15k.csv\n",
      "Dropped 0 null rows\n",
      "\n",
      "Import puppies_15k.csv\n",
      "Dropped 0 null rows\n",
      "\n",
      "Import 2022_supreme_court_tweets_15k.csv\n",
      "Dropped 0 null rows\n",
      "\n",
      "Import 2022_imac_tweets_15k.csv\n",
      "Dropped 0 null rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "for dataset in dataset_files:\n",
    "    print(f\"Import {dataset}\")\n",
    "    df = pd.read_csv(relative_import_path + dataset, sep=sep, header=header, names=names)\n",
    "    \n",
    "    if 'tweet' in df:\n",
    "        final_df = df[['tweet']].astype('string')\n",
    "    elif 'text' in df:\n",
    "        final_df = df[['text']]\n",
    "        final_df = final_df.rename(columns={\"text\":'tweet'})\n",
    "    \n",
    "    if 'sentiment' in df:\n",
    "        final_df = final_df.join(df['sentiment'])\n",
    "        \n",
    "    size_before = len(final_df)\n",
    "    final_df['tweet'].replace('', np.nan, inplace=True)\n",
    "    final_df['tweet'].replace(' ', np.nan, inplace=True) \n",
    "    final_df.dropna(inplace=True)\n",
    "    size_after = len(final_df)\n",
    "    \n",
    "    print(f'Dropped {size_before - size_after} null rows')\n",
    "    print()\n",
    "    \n",
    "    datasets.append(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ffdadbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @purinkoo: jungkookie is really the pretty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Light Painting 101: Illuminating a terrifying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Light Painting 101: Illuminating a terrifying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @Coolio_Art: Small art tip for drawing rept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @nahi_tigray: There is never true peace wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15156</th>\n",
       "      <td>We kicked the day off w/ a rocking dinosaur! S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15157</th>\n",
       "      <td>Dinosaur T-rex Colored Lights Christmas Hawaii...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15158</th>\n",
       "      <td>Dinosaur T-rex Colored Lights Christmas Xmas B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15159</th>\n",
       "      <td>#hashtag2 Cartoon cute dinosaur backpack nylon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15160</th>\n",
       "      <td>RT @EbbyElle: The Runaway Dinosaur remains one...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15161 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "0      RT @purinkoo: jungkookie is really the pretty ...\n",
       "1      Light Painting 101: Illuminating a terrifying ...\n",
       "2      Light Painting 101: Illuminating a terrifying ...\n",
       "3      RT @Coolio_Art: Small art tip for drawing rept...\n",
       "4      RT @nahi_tigray: There is never true peace wit...\n",
       "...                                                  ...\n",
       "15156  We kicked the day off w/ a rocking dinosaur! S...\n",
       "15157  Dinosaur T-rex Colored Lights Christmas Hawaii...\n",
       "15158  Dinosaur T-rex Colored Lights Christmas Xmas B...\n",
       "15159  #hashtag2 Cartoon cute dinosaur backpack nylon...\n",
       "15160  RT @EbbyElle: The Runaway Dinosaur remains one...\n",
       "\n",
       "[15161 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06776056",
   "metadata": {},
   "source": [
    "# Clean Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc149f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean dino_tweets_15k.csv\n",
      "Dropped 6350 duplicates.\n",
      "Dropped 1 empty strings. Final size 8810.\n",
      "\n",
      "Clean league_tweets_15k.csv\n",
      "Dropped 12551 duplicates.\n",
      "Dropped 0 empty strings. Final size 2609.\n",
      "\n",
      "Clean musk_tweets_15k.csv\n",
      "Dropped 11529 duplicates.\n",
      "Dropped 0 empty strings. Final size 3545.\n",
      "\n",
      "Clean more_dino_tweets_15k.csv\n",
      "Dropped 7006 duplicates.\n",
      "Dropped 1 empty strings. Final size 8064.\n",
      "\n",
      "Clean more_league_tweets_15k.csv\n",
      "Dropped 12552 duplicates.\n",
      "Dropped 0 empty strings. Final size 2457.\n",
      "\n",
      "Clean more_chief_twit_15k.csv\n",
      "Dropped 11564 duplicates.\n",
      "Dropped 1 empty strings. Final size 3486.\n",
      "\n",
      "Clean puppies_15k.csv\n",
      "Dropped 8876 duplicates.\n",
      "Dropped 1 empty strings. Final size 6126.\n",
      "\n",
      "Clean 2022_supreme_court_tweets_15k.csv\n",
      "Dropped 3481 duplicates.\n",
      "Dropped 1 empty strings. Final size 11558.\n",
      "\n",
      "Clean 2022_imac_tweets_15k.csv\n",
      "Dropped 4856 duplicates.\n",
      "Dropped 1 empty strings. Final size 10233.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset, filename) in zip(datasets, dataset_files):\n",
    "    print(f\"Clean {filename}\")\n",
    "    dataset['tweet'] = dataset['tweet'].apply(cleanTweet)\n",
    "    \n",
    "    beforeSize = len(dataset)\n",
    "    dataset.drop_duplicates(inplace = True)\n",
    "    afterSize = len(dataset)\n",
    "        \n",
    "    print(f\"Dropped {beforeSize - afterSize} duplicates.\")\n",
    "    \n",
    "    beforeSize = len(dataset)\n",
    "    dataset.drop(dataset[dataset['tweet'] == ''].index, inplace=True)\n",
    "    afterSize = len(dataset)\n",
    "    \n",
    "    print(f\"Dropped {beforeSize - afterSize} empty strings. Final size {afterSize}.\")\n",
    "    \n",
    "    dataset.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86a8d0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i asked soojin to scare me as she was a dinosaur but i actually fell in love instead'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]['tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ced71456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i asked soojin to scare me as she was a dinosa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"it was high tide, and the sound of the waves ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lots of fun on riding a dinosaur and seeing so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a grey cowled wood rail from cali . what a cut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my handsome dinosaur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8059</th>\n",
       "      <td>dinosaur hunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8060</th>\n",
       "      <td>the future swamp giant is growing steadily as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8061</th>\n",
       "      <td>ha, at least twice. how he gets a sky gig is b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8062</th>\n",
       "      <td>well i wanted to be a dinosaur when i was younger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8063</th>\n",
       "      <td>i got a glow in the dark dinosaur blanket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8064 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet\n",
       "0     i asked soojin to scare me as she was a dinosa...\n",
       "1     \"it was high tide, and the sound of the waves ...\n",
       "2     lots of fun on riding a dinosaur and seeing so...\n",
       "3     a grey cowled wood rail from cali . what a cut...\n",
       "4                                  my handsome dinosaur\n",
       "...                                                 ...\n",
       "8059                                      dinosaur hunt\n",
       "8060  the future swamp giant is growing steadily as ...\n",
       "8061  ha, at least twice. how he gets a sky gig is b...\n",
       "8062  well i wanted to be a dinosaur when i was younger\n",
       "8063          i got a glow in the dark dinosaur blanket\n",
       "\n",
       "[8064 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979d42d",
   "metadata": {},
   "source": [
    "# Export cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e715fb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export dino_tweets_15k\n",
      "Export league_tweets_15k\n",
      "Export musk_tweets_15k\n",
      "Export more_dino_tweets_15k\n",
      "Export more_league_tweets_15k\n",
      "Export more_chief_twit_15k\n",
      "Export puppies_15k\n",
      "Export 2022_supreme_court_tweets_15k\n",
      "Export 2022_imac_tweets_15k\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(relative_export_path):\n",
    "    os.mkdir(relative_export_path)\n",
    "\n",
    "for (dataset, filename) in zip(datasets, dataset_files):\n",
    "    filename, ext = os.path.splitext(filename)\n",
    "    print(f\"Export {filename}\")\n",
    "    dataset.to_csv(relative_export_path + filename + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa407e7",
   "metadata": {},
   "source": [
    "# Export final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2af2737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat(datasets)\n",
    "dataset.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24fd3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(relative_export_path + 'data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "783a69a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Size: 56888\n"
     ]
    }
   ],
   "source": [
    "print(f'Final Dataset Size: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4868e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
